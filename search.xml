<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[爬取百度搜索]]></title>
    <url>%2F2017%2F09%2F01%2F%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[在用谷歌语法搜索有某些特征的链接时，如果想把这些链接全部保存起来，这个时候就可以使用爬虫技术，爬取这些链接保存下来。下面就来分析并写出这个爬虫程序。 网页分析分析搜索链接每页的网页链接格式，一般都有固定的链接格式，如百度的每页搜索结果链接是只取两个个参数的结果是这样，每页10条1https://www.baidu.com/s?wd=ctf&amp;pn=10 分析搜索页面中的链接F12对当前页面分析每个链接的特点，百度搜索有点坑，你会发现百度都是通过一个长长的链接302跳转来访问的，随便选取一个链接都是这种1a target=&quot;_blank&quot; href=&quot;你搜索的URL&quot; class=&quot;c-showurl&quot; style=&quot;text-decoration:none;&quot;&gt;www.php.net/downloa...php 特征就是class=”c-showurl” 属性值，用bs库去获取所有有这个属性的tagres = soup.find_all(name=”a”, attrs={‘class’:’c-showurl’}) 访问链接访问跳转链接获取实际网站url,title之类的信息 爬虫实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120#!/usr/bin/env python#coding=utf-8#输入格式 python 脚本 -s 内容 -f 要保存的文件名#每页的网页链接格式，一般都有固定的链接格式，如百度的每页搜索结果链接是只取两个个参数的结果是这样，每页10条#https://www.baidu.com/s?wd=ctf&amp;pn=10#F12对当前页面分析每个链接的特点，百度搜索有点坑，你会发现百度都是通过一个长长的链接302跳转来访问的，随便选取一个链接都是这种#&lt;a target="_blank" href="http://www.baidu.com/link?url=GI9K125i3rnLbxL2-kKs-2g2OZt-oDTJZZIFjndQHxGiDubfIEpvNxnnCc1h5ags" class="c-showurl" style="text-decoration:none;"&gt;www.secbox.cn/tag/&lt;b&gt;ctf&lt;/b&gt;&amp;nbsp;&lt;/a&gt;import requests from bs4 import BeautifulSoup as bsimport threading #多线程import re #正则from Queue import Queue #线程优先级队列（ Queue）from prettytable import PrettyTable #将输出内容如表格方式整齐 import argparse #命令行解析import timeimport systhread_count = 3 #进程数page = 5 #可以修改抓取页数urls = []table = PrettyTable(['page','url','title']) #prettyx模块将输出内容如表格方式整齐table.align['title'] = '1' #title左对齐table.padding_width = 1 #列边和内容之间的一个空格page = (page+1) * 10class mythread(threading.Thread): #继承父类threading.Thread def __init__(self,queue): threading.Thread.__init__(self) self.Q = queue self.headers = &#123;'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'&#125; #设置请求头 def run(self): ##把要执行的代码写到run函数里面 线程在创建后会直接运行run函数 while 1: try: t = self.Q.get(True,1) #print t self.spider(t) except Exception,e: #调试最好打印出错信息，否则，spider函数出错也无法定位错误，多次遇到这个问题了,靠打印才解决 print e break def spider(self,target): #爬取网页链接和标题 #print type(target) pn =int(target.split('=')[-1])/10 + 1 #对https://www.baidu.com/s?wd=ctf&amp;pn=10分割去最后的数字 #print pn #print target html = requests.get(target,headers=self.headers) #print html soup = bs(html.text,'lxml') res = soup.find_all(name='a', attrs=&#123;'class':'c-showurl'&#125;) #print res for r in res: try: #因为百度搜索是302跳转，所以我们需要再次请求 h = requests.get(r['href'],headers=self.headers,timeout=3) if h.status_code == 200: url = h.url title =re.findall(r'&lt;title&gt;(.*?)&lt;/title&gt;',h.content)[0] title = title.decode('utf-8') #解码成unicode,否则add_row会转换出错 urls.append((pn,url,title)) else: continue except: continuedef Load_Thread(queue): #生成线程数 return [mythread(queue) for i in range(thread_count)]def Start_Thread(threads): print 'thread is start...' for t in threads: t.setDaemon(True) t.start() for t in threads: t.join() print 'thread is end...'def main(): start = time.time() parser = argparse.ArgumentParser() parser.add_argument('-s') parser.add_argument('-f') arg = parser.parse_args() #print arg word = arg.s output = arg.f # word = 'inurl:login.action' # output = 'test.txt' queue = Queue() for i in range(0,page,10): target = 'https://www.baidu.com/s?wd=%s&amp;pn=%s'%(word,i) queue.put(target) thread_list = Load_Thread(queue) Start_Thread(thread_list) #把数据写到文件中 if output: with open(output,'a') as f: for record in urls: f.write(record[1]+'\n') #print urls,len(urls) for record in urls: table.add_row(list(record)) #在表单中添加数据 print table print '共爬取数据%s条'%len(urls) print time.time()-startif __name__ == '__main__': main()]]></content>
      <categories>
        <category>爬虫技术</category>
      </categories>
      <tags>
        <tag>python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试常用脑图]]></title>
    <url>%2F2017%2F08%2F31%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%B8%B8%E7%94%A8%E8%84%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[渗透测试脑图 漏洞脑图web常见漏洞脑图 xss攻击点汇总脑图 密码找回逻辑漏洞脑图 越权脑图 工具脑图sqlmap脑图 nmap脑图 提权脑图]]></content>
      <categories>
        <category>web安全</category>
      </categories>
      <tags>
        <tag>脑图</tag>
        <tag>漏洞</tag>
      </tags>
  </entry>
</search>
