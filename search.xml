<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[服务器解析漏洞]]></title>
    <url>%2F2017%2F09%2F01%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A7%A3%E6%9E%90%E6%BC%8F%E6%B4%9E%2F</url>
    <content type="text"><![CDATA[服务器解析漏洞已经是一个老生常谈问题了，但是有些服务器依然存在，这里复习记录一下常见服务器解析漏洞，比如IIS6.0，IIS7.5，acache，nginx解析漏洞。 一.IIS5.x-6.x解析漏洞使用iis5.x-6.x版本的服务器，大多为windows server 2003，网站比较古老，开发语句一般为asp；该解析漏洞也只能解析asp文件，而不能解析aspx文件 目录解析(6.0)形式：www.xxx.com/xx.asp/xx.jpg原理: 服务器默认会把.asp，.asa目录下的文件都解析成asp文件。 文件解析形式：www.xxx.com/xx.asp;.jpg原理：服务器默认不解析;号后面的内容，因此xx.asp;.jpg便被解析成asp文件了。 解析文件类型IIS6.0 默认的可执行文件除了asp还包含这三种 :/test.asa/test.cer/test.cdx 修复方案1.禁止用户控制文件上传目录，新建目录等权限2.上传目录与用户新建的目录禁止执行3.上传的文件重命名，不保留用户上传文件的后缀4.禁止asa、asp、cer、cdx等后缀的文件上传 二.apache解析漏洞漏洞原理 Apache 解析文件的规则是从右到左开始判断解析,如果后缀名为不可识别文件解析,就再往左判断。比如 test.php.owf.rar “.owf”和”.rar” 这两种后缀是apache不可识别解析,apache就会把wooyun.php.owf.rar解析成php。 漏洞形式www.xxxx.xxx.com/test.php.php123 其余配置问题导致漏洞（1）如果在 Apache 的 conf 里有这样一行配置 AddHandler php5-script .php 这时只要文件名里包含.php 即使文件名是 test2.php.jpg 也会以 php 来执行。（2）如果在 Apache 的 conf 里有这样一行配置 AddType application/x-httpd-php .jpg 即使扩展名是 jpg，一样能以 php 方式执行 修复方案1.apache配置文件，禁止.php.这样的文件执行，配置文件里面加入1234&lt;Files ~ “.(php.|php3.)”&gt;Order Allow,DenyDeny from all&lt;/Files&gt; 2.用伪静态能解决这个问题，重写类似.php.*这类文件，打开apache的httpd.conf找到LoadModule rewrite_module modules/mod_rewrite.so把#号去掉，重启apache,在网站根目录下建立.htaccess文件,代码如下1234567891011&lt;IfModule mod_rewrite.c&gt;RewriteEngine OnRewriteRule .(php.|php3.) /index.phpRewriteRule .(pHp.|pHp3.) /index.phpRewriteRule .(phP.|phP3.) /index.phpRewriteRule .(Php.|Php3.) /index.phpRewriteRule .(PHp.|PHp3.) /index.phpRewriteRule .(PhP.|PhP3.) /index.phpRewriteRule .(pHP.|pHP3.) /index.phpRewriteRule .(PHP.|PHP3.) /index.php&lt;/IfModule&gt; 三.Nginx &lt; 8.03畸形解析漏洞漏洞原理Nginx默认是以CGI的方式支持PHP解析的，普遍的做法是在Nginx配置文件中通过正则匹配设置SCRIPT_FILENAME。当访问www.xx.com/phpinfo.jpg/1.php这个URL时，$fastcgi_script_name会被设置为“phpinfo.jpg/1.php”，然后构造成SCRIPT_FILENAME传递给PHP CGI，但是PHP为什么会接受这样的参数，并将phpinfo.jpg作为PHP文件解析呢?这就要说到fix_pathinfo这个选项了。 如果开启了这个选项，那么就会触发在PHP中的如下逻辑：PHP会认为SCRIPT_FILENAME是phpinfo.jpg，而1.php是PATH_INFO，所以就会将phpinfo.jpg作为PHP文件来解析了 漏洞形式www.xxxx.com/UploadFiles/image/1.jpg/1.phpwww.xxxx.com/UploadFiles/image/1.jpg%00.phpwww.xxxx.com/UploadFiles/image/1.jpg/%20\0.php 另外一种手法：上传一个名字为test.jpg，以下内容的文件。1&lt;?PHP fputs(fopen(&apos;shell.php&apos;,&apos;w&apos;),&apos;&lt;?php eval($_POST[cmd])?&gt; 然后访问test.jpg/.php,在这个目录下就会生成一句话木马shell.php。 修复方案1.修改php.ini文件，将cgi.fix_pathinfo的值设置为0;2.在Nginx配置文件中添加以下代码：1234if ( $fastcgi_script_name ~ ..*/.*php ) &#123; return 403; &#125; 这行代码的意思是当匹配到类似test.jpg/a.php的URL时，将返回403错误代码。3.升级Nignx到最新版 四.IIS7.5解析漏洞IIS7.5的漏洞与nginx的类似，都是由于php配置文件中，开启了cgi.fix_pathinfo，而这并不是nginx或者iis7.5本身的漏洞。]]></content>
      <categories>
        <category>web安全</category>
      </categories>
      <tags>
        <tag>服务器解析漏洞</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取百度搜索]]></title>
    <url>%2F2017%2F09%2F01%2F%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[在用谷歌语法搜索有某些特征的链接时，如果想把这些链接全部保存起来，这个时候就可以使用爬虫技术，爬取这些链接保存下来。下面就来分析并写出这个爬虫程序。 网页分析分析搜索链接每页的网页链接格式，一般都有固定的链接格式，如百度的每页搜索结果链接是只取两个个参数的结果是这样，每页10条1https://www.baidu.com/s?wd=ctf&amp;pn=10 分析搜索页面中的链接F12对当前页面分析每个链接的特点，百度搜索有点坑，你会发现百度都是通过一个长长的链接302跳转来访问的，随便选取一个链接都是这种1a target=&quot;_blank&quot; href=&quot;你搜索的URL&quot; class=&quot;c-showurl&quot; style=&quot;text-decoration:none;&quot;&gt;www.php.net/downloa...php 特征就是class=”c-showurl” 属性值，用bs库去获取所有有这个属性的tagres = soup.find_all(name=”a”, attrs={‘class’:’c-showurl’}) 访问链接访问跳转链接获取实际网站url,title之类的信息 爬虫实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120#!/usr/bin/env python#coding=utf-8#输入格式 python 脚本 -s 内容 -f 要保存的文件名#每页的网页链接格式，一般都有固定的链接格式，如百度的每页搜索结果链接是只取两个个参数的结果是这样，每页10条#https://www.baidu.com/s?wd=ctf&amp;pn=10#F12对当前页面分析每个链接的特点，百度搜索有点坑，你会发现百度都是通过一个长长的链接302跳转来访问的，随便选取一个链接都是这种#&lt;a target="_blank" href="http://www.baidu.com/link?url=GI9K125i3rnLbxL2-kKs-2g2OZt-oDTJZZIFjndQHxGiDubfIEpvNxnnCc1h5ags" class="c-showurl" style="text-decoration:none;"&gt;www.secbox.cn/tag/&lt;b&gt;ctf&lt;/b&gt;&amp;nbsp;&lt;/a&gt;import requests from bs4 import BeautifulSoup as bsimport threading #多线程import re #正则from Queue import Queue #线程优先级队列（ Queue）from prettytable import PrettyTable #将输出内容如表格方式整齐 import argparse #命令行解析import timeimport systhread_count = 3 #进程数page = 5 #可以修改抓取页数urls = []table = PrettyTable(['page','url','title']) #prettyx模块将输出内容如表格方式整齐table.align['title'] = '1' #title左对齐table.padding_width = 1 #列边和内容之间的一个空格page = (page+1) * 10class mythread(threading.Thread): #继承父类threading.Thread def __init__(self,queue): threading.Thread.__init__(self) self.Q = queue self.headers = &#123;'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'&#125; #设置请求头 def run(self): ##把要执行的代码写到run函数里面 线程在创建后会直接运行run函数 while 1: try: t = self.Q.get(True,1) #print t self.spider(t) except Exception,e: #调试最好打印出错信息，否则，spider函数出错也无法定位错误，多次遇到这个问题了,靠打印才解决 print e break def spider(self,target): #爬取网页链接和标题 #print type(target) pn =int(target.split('=')[-1])/10 + 1 #对https://www.baidu.com/s?wd=ctf&amp;pn=10分割去最后的数字 #print pn #print target html = requests.get(target,headers=self.headers) #print html soup = bs(html.text,'lxml') res = soup.find_all(name='a', attrs=&#123;'class':'c-showurl'&#125;) #print res for r in res: try: #因为百度搜索是302跳转，所以我们需要再次请求 h = requests.get(r['href'],headers=self.headers,timeout=3) if h.status_code == 200: url = h.url title =re.findall(r'&lt;title&gt;(.*?)&lt;/title&gt;',h.content)[0] title = title.decode('utf-8') #解码成unicode,否则add_row会转换出错 urls.append((pn,url,title)) else: continue except: continuedef Load_Thread(queue): #生成线程数 return [mythread(queue) for i in range(thread_count)]def Start_Thread(threads): print 'thread is start...' for t in threads: t.setDaemon(True) t.start() for t in threads: t.join() print 'thread is end...'def main(): start = time.time() parser = argparse.ArgumentParser() parser.add_argument('-s') parser.add_argument('-f') arg = parser.parse_args() #print arg word = arg.s output = arg.f # word = 'inurl:login.action' # output = 'test.txt' queue = Queue() for i in range(0,page,10): target = 'https://www.baidu.com/s?wd=%s&amp;pn=%s'%(word,i) queue.put(target) thread_list = Load_Thread(queue) Start_Thread(thread_list) #把数据写到文件中 if output: with open(output,'a') as f: for record in urls: f.write(record[1]+'\n') #print urls,len(urls) for record in urls: table.add_row(list(record)) #在表单中添加数据 print table print '共爬取数据%s条'%len(urls) print time.time()-startif __name__ == '__main__': main()]]></content>
      <categories>
        <category>爬虫技术</category>
      </categories>
      <tags>
        <tag>python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试常用脑图]]></title>
    <url>%2F2017%2F08%2F31%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%B8%B8%E7%94%A8%E8%84%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[渗透测试脑图 漏洞脑图web常见漏洞脑图 xss攻击点汇总脑图 密码找回逻辑漏洞脑图 越权脑图 工具脑图sqlmap脑图 nmap脑图 提权脑图]]></content>
      <categories>
        <category>web安全</category>
      </categories>
      <tags>
        <tag>脑图</tag>
        <tag>漏洞</tag>
      </tags>
  </entry>
</search>
